{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install llama-index llama-index-llms-mistralai llama-index-embeddings-fastembed -q\n%pip install llama-index-retrievers-bm25 llama-index-tools-duckduckgo -q","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.core.agent.workflow import AgentWorkflow\nfrom llama_index.core import Settings\nfrom llama_index.llms.mistralai import MistralAI \nfrom llama_index.embeddings.fastembed import FastEmbedEmbedding\nimport os\n\nos.environ[\"MISTRAL_API_KEY\"] = \"API KEY\"\n\nSettings.llm = MistralAI(model=\"mistral-small-latest\")\nSettings.embed_model = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en\")\nSettings.chunk_size = 512\nSettings.chunk_overlap = 64","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import datasets\nfrom llama_index.core.schema import Document","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"docs = [\n    Document(\n        text=\"\\n\".join([\n            f\"Name: {guest_dataset['name'][i]}\",\n            f\"Relation: {guest_dataset['relation'][i]}\",\n            f\"Description: {guest_dataset['description'][i]}\",\n            f\"Email: {guest_dataset['email'][i]}\"\n        ]),\n        metadata={\"name\": guest_dataset['name'][i]}\n    )\n    for i in range(len(guest_dataset))\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec\nfrom llama_index.core.tools import FunctionTool\n\ntool_spec = DuckDuckGoSearchToolSpec()\n\nsearch_tool = FunctionTool.from_defaults(tool_spec.duckduckgo_full_search)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom llama_index.core.tools import FunctionTool\nfrom huggingface_hub import list_models, list_datasets\n\ndef get_hub_stats(author: str) -> str:\n    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n    try:\n        \n        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n\n        if models:\n            model = models[0]\n            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n        else:\n            return f\"No model found for author {author}.\"\n    except Exception as e:\n        return f\"Error fetching dataset for {author}: {str(e)}\"\n\n \nhub_stats_tool = FunctionTool.from_defaults(get_hub_stats)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\nfrom llama_index.core.tools import FunctionTool\n\ndef get_weather_info(location: str) -> str:\n    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n    weather_conditions = [\n        {\"condition\": \"Rainy\", \"temp_c\": 15},\n        {\"condition\": \"Clear\", \"temp_c\": 25},\n        {\"condition\": \"Windy\", \"temp_c\": 20}\n    ]\n    data = random.choice(weather_conditions)\n    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}Â°C\"\n\nweather_info_tool = FunctionTool.from_defaults(get_weather_info)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.core.tools import FunctionTool\nfrom llama_index.retrievers.bm25 import BM25Retriever\n\nbm25_retriever = BM25Retriever.from_defaults(nodes=docs)\n\ndef get_guest_info_retriever(query: str) -> str:\n    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n    results = bm25_retriever.retrieve(query)\n    if results:\n        return \"\\n\\n\".join([doc.text for doc in results[:3]])\n    else:\n        return \"No matching guest information found.\"\n\nguest_info_tool = FunctionTool.from_defaults(get_guest_info_retriever)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alfred = AgentWorkflow.from_tools_or_functions(\n    [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool],\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"Tell me about Lady Ada Lovelace. What's her background?\"\nresponse = await alfred.run(query)\n\nprint(\"Response:\")\nprint(response.response.blocks[0].text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\"\nresponse = await alfred.run(query)\n\nprint(\"Response:\")\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"One of our guests is from Google. What can you tell me about their most popular model?\"\nresponse = await alfred.run(query)\n\nprint(\"Alfred's Response:\")\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = \"I need to speak with Dr. Nikola Tesla about recent advancements in wireless energy. Can you help me prepare for this conversation?\"\nresponse = await alfred.run(query)\n\nprint(\"Alfred's Response:\")\nprint(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from llama_index.core.workflow import Context\n\nctx = Context(alfred)\n\n\nresponse1 = await alfred.run(\"Tell me about Lady Ada Lovelace.\", ctx=ctx)\nprint(\"First Response:\")\nprint(response1)\n\n\nresponse2 = await alfred.run(\"What projects is she currently working on?\", ctx=ctx)\nprint(\"Second Response:\")\nprint(response2)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}